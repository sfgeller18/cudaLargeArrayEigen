#ifndef MATMUL_HPP
#define MATMUL_HPP

    #include "vector.hpp"
    #include <stdexcept>
    #include "errormsg.hpp"
    #include <cmath>
    #include <iostream>
    #include <variant>

    #include "cuda_manager.hpp"

inline void matmul_internal(const Matrix& M, DevicePrecision* d_M, const DevicePrecision* d_y, DevicePrecision* d_result, size_t ROWS, cublasHandle_t& handle) {
    size_t N, L;
    std::tie(L, N) = shape(M);
    size_t idx = 0;
    const DevicePrecision alpha = 1.0;
    const DevicePrecision beta = 0.0;
    
    while (idx < L) {
        size_t selectedRows = std::min(ROWS, L - idx);
        std::cout << "Block Transfer " << selectedRows << std::endl;
        // Copy data to device
        cudaMemcpyChecked(d_M, M.data() + idx * N, selectedRows * N * PRECISION_SIZE, cudaMemcpyKind::cudaMemcpyHostToDevice);
        cublasStatus_t status = cublasGemv(handle, CUBLAS_OP_T, N, selectedRows, &alpha, d_M, N, d_y, 1, &beta, d_result + idx, 1);
        if (status != CUBLAS_STATUS_SUCCESS) {
            throw std::runtime_error("CUBLAS GEMV failed");
        }
        idx += selectedRows;
    }
    cublasDestroy(handle);
}

Vector matmul(const Matrix& M, const Vector& y) {
    CHECK_DIMS(M, y);
    size_t free_mem = 0;
    size_t total_mem = 0;
    cudaError_t error = cudaMemGetInfo(&free_mem, &total_mem);
    size_t N = 0; // Number of columns in M
    size_t L = 0; // Number of rows in M
    // CuBLAS GEMV variables
    const DevicePrecision alpha = 1.0;
    const DevicePrecision beta = 0.0;
    size_t idx = 0;
    #ifdef USE_EIGEN
        N = M.cols();
        L = M.rows();
    #else
        N = cols(M);
        L = rows(M);
    #endif
    size_t ROWS = MAX_ROW_ALLOC(free_mem, N);
    std::cout << "Max Num Rows is " << ROWS << std::endl;
    // Allocate device memory for M, y, and result
    HostPrecision* d_M = cudaMallocChecked<DevicePrecision>(ROWS * N * PRECISION_SIZE);
    HostPrecision* d_y = cudaMallocChecked<DevicePrecision>(N * PRECISION_SIZE);
    HostPrecision* d_result = cudaMallocChecked<DevicePrecision>(L * PRECISION_SIZE);
    Vector h_result(L);
    
    cublasHandle_t handle;
    cublasCreate(&handle);
    size_t transfer_num = 0;


    cudaMemcpyChecked(d_y, y.data(), N * PRECISION_SIZE, cudaMemcpyKind::cudaMemcpyHostToDevice);
    while (idx < L) {
        size_t selectedRows = std::min(ROWS, L - idx);
        std::cout << "Block Transfer " << selectedRows << std::endl;
        // Copy data to device
        cudaMemcpyChecked(d_M, M.data() + idx * N, selectedRows * N * PRECISION_SIZE, cudaMemcpyKind::cudaMemcpyHostToDevice);
        cublasStatus_t status = cublasGemv(handle, CUBLAS_OP_T, N, selectedRows, &alpha, d_M, N, d_y, 1, &beta, d_result + idx, 1);
        idx += selectedRows;
    }
    // Return result to host result holder
    cudaMemcpyChecked(h_result.data(), d_result, L * PRECISION_SIZE, cudaMemcpyKind::cudaMemcpyDeviceToHost);
    // Free device memory
    cudaFree(d_M);
    cudaFree(d_y);
    cudaFree(d_result);
    cublasDestroy(handle);
    return h_result; // Return the resulting vector
}








#endif //MATMUL_HPP

