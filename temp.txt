#ifndef ARNOLDI_HPP
#define ARNOLDI_HPP

#include <array>
#include <tuple>
#include <iostream>
#include <vector>
#include "matmul.hpp"
#include "cuda_manager.hpp"
#include "vector.hpp"

constexpr size_t MAX_EVEC_ON_DEVICE = 1000;

struct EigenPair { Vector vector; HostPrecision value; };

template <size_t Size>
std::array<EigenPair, Size> arnoldiEigen(const Matrix& M, const size_t& max_iters, const HostPrecision& tol) {
    size_t N, L; //N=num_rows, L=num_cols
    std::tie(N, L) = shape(M);
    Vector v0 = randVecGen(N);
    Vector h_evals(max_iters);
    h_evals[0] = norm(v0);
    for (HostPrecision v : v0) {v /= h_evals[0];}      //v0 is a norm 1 random vector
    print(v0);

    size_t free_mem = 0;
    size_t total_mem = 0;
    cudaError_t error = cudaMemGetInfo(&free_mem, &total_mem);
    const size_t NUM_EVECS_ON_DEVICE = std::min(MAX_EVEC_ON_DEVICE, max_iters);
    size_t ROWS = MAX_ROW_ALLOC(free_mem, N);

    size_t m = 1;


    DevicePrecision* d_evecs = cudaMallocChecked<DevicePrecision>(NUM_EVECS_ON_DEVICE * N * PRECISION_SIZE);
    DevicePrecision* d_proj = cudaMallocChecked<DevicePrecision>(NUM_EVECS_ON_DEVICE * PRECISION_SIZE);
    DevicePrecision* d_y = cudaMallocChecked<DevicePrecision>(N * PRECISION_SIZE);
    DevicePrecision* d_M = cudaMallocChecked<DevicePrecision>(ROWS * N * PRECISION_SIZE);
    DevicePrecision* d_result = cudaMallocChecked<DevicePrecision>(L * PRECISION_SIZE);
    DevicePrecision* d_h = cudaMallocChecked<DevicePrecision>(max_iters * PRECISION_SIZE);
    
    cudaMemcpyChecked(d_y, v0.data(), N * PRECISION_SIZE, cudaMemcpyHostToDevice);
    cudaMemcpyChecked(d_evecs, v0.data(), N * sizeof(DevicePrecision), cudaMemcpyHostToDevice); //Initialize first col of evecs to v0

    cublasHandle_t handle;
    cublasCreate(&handle);

    constexpr DevicePrecision alpha = 1.0;
    constexpr DevicePrecision beta = 0.0;
    constexpr DevicePrecision neg_one = -1.0;


for (int i = 1; i < max_iters; i++) {
    std::cout << "New Iter" << std::endl;
    
    // Perform matrix multiplication
    matmul_internal(M, d_M, d_y, d_result, ROWS, handle);

    // Transfer d_result to host and print
    // Vector h_result(L);
    // std::cout << "Result after Matmul" << std::endl;
    // cudaMemcpyChecked(h_result.data(), d_result, L * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // cudaMemcpyChecked(h_result.data(), d_result, L * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // print(h_result);

    // Perform GEMV operation
    cublasGemv(handle, CUBLAS_OP_T, i, N, &alpha, d_evecs, N, d_result, 1, &beta, d_h, 1);

    // std::cout << "Evecs after Gemv" << std::endl;
    // MatrixColMajor h_evecs(N, NUM_EVECS_ON_DEVICE);
    // cudaMemcpyChecked(h_evecs.data(), d_evecs, NUM_EVECS_ON_DEVICE * L * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // print(h_evecs);

    // Perform SGER operation
    cublasSger(handle, N, i, &neg_one, d_result, 1, d_evecs, 1, d_h, N);

    // Transfer d_result to h_result and print again
    // std::cout << "Results after Sger" << std::endl;
    // cudaMemcpyChecked(h_result.data(), d_result, L * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // print(h_result);

    // Compute the norm and update the eigenvalues
    cublasSnrm2(handle, L, d_result, 1, &h_evals[i]);  // eval = ||y||

    // std::cout << "Evals after norm" << std::endl;
    // print(h_evals); 

    // Normalize d_result
    float inv_eval = 1.0f / h_evals[i];
    cublasSscal(handle, N, &inv_eval, d_result, 1);  // y /= eval

    // Copy the normalized result to the eigenvector array
    cudaMemcpyChecked(&d_evecs[i * N], d_result, N * sizeof(float), cudaMemcpyDeviceToDevice);
    // std::cout << "Evecs after Normalization" << std::endl;
    // cudaMemcpyChecked(h_evecs.data(), d_evecs, NUM_EVECS_ON_DEVICE * L * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // print(h_evecs);

    cudaMemcpyChecked(d_y, d_result, N * sizeof(HostPrecision), cudaMemcpyDeviceToDevice);
    // std::cout << "y after iter" << std::endl;
    // cudaMemcpyChecked(v0.data(), d_y, N * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // print(v0);
    m++;

    // Transfer d_y to the host for potential further use
    // cudaMemcpyChecked(v0.data(), d_y, N * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    // print(v0);
    // Check for tolerance
    if (h_evals[i] < tol) {
        std::cout << "Break" << std::endl;
        break;
    } 
}

    print(h_evals);


    std::cout << "M: " << m << std::endl;

    // Copy evals and evecs back to host
    MatrixColMajor h_evecs(L, m);
    cudaMemcpyChecked(h_evecs.data(), d_evecs, NUM_EVECS_ON_DEVICE * N * sizeof(HostPrecision), cudaMemcpyDeviceToHost);
    print(h_evecs);

    // Sort eigenvalues and corresponding eigenvectors
    std::vector<size_t> sorted_idx(m);
    std::iota(sorted_idx.begin(), sorted_idx.end(), 0);
    std::sort(sorted_idx.begin(), sorted_idx.end(), [&](size_t a, size_t b) { return h_evals[a] > h_evals[b]; });

    

    // Select top eigenpairs
    std::array<EigenPair, Size> result;
    for (size_t i = 0; i < m; ++i) {
        size_t idx = sorted_idx[i];
        result[i].value = h_evals[idx];
        result[i].vector.resize(N);
        result[i].vector = h_evecs.block(0, idx, N, 1);
    }

    std::cout << m << std::endl;
    // Free device and host memory
    cudaFree(d_evecs);
    cudaFree(d_proj);
    cudaFree(d_y);
    cudaFree(d_M);
    cudaFree(d_result);
    cudaFree(d_h);

    cublasDestroy(handle);

    return result;
}

#endif // ARNOLDI_HPP


template <typename MatrixGeneric>
inline int matmul_internal(const MatrixGeneric& M, DevicePrecision* d_M, const DevicePrecision* d_y, DevicePrecision* d_result, size_t ROWS, cublasHandle_t& handle) {
    constexpr bool isRowMajor = MatrixGeneric::IsRowMajor;
    size_t N, L;
    std::tie(N, L) = shape(M);  // Assuming shape returns (num_rows, num_cols)
    std::cout << "Matrix shape: " << N << "x" << L << std::endl;
    std::cout << "ROWS: " << ROWS << std::endl;

    size_t idx = 0;
    constexpr DevicePrecision alpha = 1.0;
    constexpr DevicePrecision beta = 0.0;
    size_t& iterIndSize = (isRowMajor) ? N : L;
    size_t& dataColSize = (isRowMajor) ? L : N;
   
    while (idx < iterIndSize) {
        size_t selectedRows = std::min(ROWS, iterIndSize - idx);
        std::cout << "Block Transfer " << selectedRows << " rows, starting from index " << idx << std::endl;
        
        cudaMemcpyChecked(d_M, M.data() + idx * dataColSize, selectedRows * dataColSize * PRECISION_SIZE, cudaMemcpyKind::cudaMemcpyHostToDevice);

        if (isRowMajor) {
            CHECK_CUBLAS(cublasGemv(handle, CUBLAS_OP_N, dataColSize, selectedRows, &alpha, d_M, L, d_y + idx, 1, &beta, d_result, 1));
        } else {
            CHECK_CUBLAS(cublasGemv(handle, CUBLAS_OP_N, dataColSize, selectedRows, &alpha, d_y, L, d_M + (idx * iterIndSize), 1, &beta, d_result, 1));
        }
        CHECK_CUDA(cudaDeviceSynchronize());

        idx += selectedRows;
    }
    return EXIT_SUCCESS;
}
